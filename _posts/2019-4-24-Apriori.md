---
layout: post
title: "Understanding Market Basket Analysis with Apriori"
date: 2019-04-02
excerpt: "A ton of text to understand Natural Language Processing."
tags: [Market Basket Analysis, Association rule mining, Apriori, R]
comments: true
---

# Apriori 
Apriori is a common algorithms that is used for frequent item set mining and association rule learning. It is a very effective Market Basket Analysis technique. In Retail scenarios, it heelps customers in purchasing theri itemsets with more ease while also increasing saled. Apriori is known to be used in healthcare, manufacturing and many other domains.

Formally, Apriori has been defined as follows

Let I={i<sub>1</sub>,i<sub>2</sub>,i<sub>3</sub>,…,i<sub>n</sub>} be a set of n attributes called items and D={t<sub>1</sub>,t<sub>2</sub>,…,t<sub>n</sub>} be the set of transactions. It is called database. Every transaction, ti in D has a unique transaction ID, and it consists of a subset of itemsets in I.
A rule can be defined as an implication, X⟶Y where X and Y are subsets of I(X,Y⊆I), and they have no element in common, i.e., X∩Y. X and Y are the antecedent and the consequent of the rule, respectively.

Consider the follwing scenario:

Here the set of items I = {Choclates, Fruits, Bread, Milk, Eggs}. 

An example for a rule here would be {Choclates, fruits} => {Bread}

| Transaction ID 	| 	Choclates 	| 			Fruits 	| Bread 	| Milk 	| Eggs 	|
|----------------	|-----------	|--------	|-------	|------	|------	|
| t1             	| 1         	| 1      	| 1     	| 0    	| 0    	|
| t2             	| 0         	| 1      	| 1     	| 1    	| 0    	|
| t3             	| 0         	| 0      	| 1     	| 1    	| 1    	|
| t4             	| 1         	| 1      	| 0     	| 1    	| 0    	|
| t5             	| 1         	| 1      	| 1     	| 0    	| 1    	|
| t6             	| 1         	| 1      	| 1     	| 1    	| 1    	|

There are multiple rules that are possible here. In order to calculate these rules, 3 measures are particularly useful

## Support

The support of an itemset X, support(X) is the proportion of transaction in the database in which the item X appears. It signifies the popularity of an itemset.

support(X) = Number of transaction in which X appears / Total number of transactions.

For example, Support(Choclates) = 4/6 = 0.66667

## Confidence

This measure says how likely item Y is purchased when item X is purchased. 

Confidence (X -> Y) = Support (X ∪ Y) / (1- confidence (X⟶Y)) 

## Lift

This says how likely item Y is purchased when item X is purchased, while controlling for how popular item Y is. A lift value of 1 means no association between items. A lift value greater than 1 means that item Y is likely to be bought if item X is bought, while a value less than 1 means that item Y is unlikely to be bought if item X is bought, while a value less than 1 means that item Y is unlikely to be bought if item X is bought

lift( X⟶Y )=support( X∪Y )/ support( X ) ∗ support( Y )

## Conviction
The conviction of X⟶Y is the inverse lift of the rule X⟶not Y. 
The conviction states by what factor the correctness of the rule (as expressed by its confidence) would reduce if the antecedent and the consequent of the rule were independent. A high value therefore means that the consequent depends strongly on the antecedent.
The Conviction rule can be defined as
conviction( X⟶Y ) =1−support( Y) / 1−confidence( X⟶Y )

## How apriori actually works

Apriori algorithm assumes anti-monotonicity of the support measure. This means

1. All subsets of a frequent itemset must be frequent
2. Similarly, for any infrequent itemset, all its supersets must be infrequent too.

<b>Step 1<b>: Create a frequency table of all the items that occur in all the transactions. 

| Item           	| Frequency 	|
|----------------	|-----------	|
| Chocolates (C) 	| 4         	|
| Fruits(F)      	| 5         	|
| Bread(B)       	| 4         	|
| Milk(M)        	| 4         	|
| Eggs(E)        	| 2         	|

<b> Step 2: </b> Only those elements whose support it greater than equal to a threshold support are significant. Suppose the support threshold is 50%. We are left with 
| Item           	| Frequency 	|
|----------------	|-----------	|
| Chocolates (C) 	| 4         	|
| Fruits(F)      	| 5         	|
| Bread(B)       	| 4         	|
| Milk(M)        	| 4         	|

<b> Step 3: </b> Create all possible pairs of the significant items. Here order does not matter, AB would be considered equal to BA. So the pairs in our set would be, CF, CB, CM, FB, FM, BM. 

<b> Step 4: </b> We count the occurences of each pair in these transactions.

| Itemset 	| Frequency 	|
|---------	|-----------	|
| CF      	| 4         	|
| CB      	| 3         	|
| CM      	| 2         	|
| FB      	| 4         	|
| FM      	| 3         	|
| BM      	| 2         	|


<b> Step 5: </b> Choose those itemsets whose support crosses the significant thereshold. This leaves us with CF, CB, FB and FM. 

## Generic Process of the apriori algorithm

The entire algorithm can be divided into two steps:

Step 1: Apply minimum support to find all the frequent sets with k items in a database.

Step 2: Use the self-join rule to find the frequent sets with k+1 items with the help of frequent k-itemsets. Repeat this process from k=1 to the point when we are unable to apply the self-join rule.

This approach of extending a frequent itemset one at a time is called the “bottom up” approach.

## Mining association rules

For finding association rules, we need to find all rules having support greater than the threshold support and confidence greater than the threshold confidence. One possible way is brute force, i.e., to list all the possible association rules and calculate the support and confidence for each rule. Then eliminate the rules that fail the threshold support and confidence. But it is computationally very heavy and prohibitive as the number of all the possible association rules increase exponentially with the number of items.

Given there are n items in the set I, the total number of possible association rules is 3<sup>n</sup>–2<sup>n+1</sup>+1.

We can also use a two-step approach

Step 1: Frequent itemset generation: Find all itemsets for which the support is greater than the threshold support following the process we have already seen earlier in this article.

Step 2: Rule generation: Create rules from each frequent itemset using the binary partition of frequent itemsets and look for the ones with high confidence. These rules are called candidate rules.

If X is a frequent itemset with k elements, then there are 2<sup>k</sup>−2 candidate association rules.

## Pros of the Apriori algorithm

* It is an easy-to-implement and easy-to-understand algorithm.
* It can be used on large itemsets.

## Cons of the Apriori Algorithm

* Sometimes, it may need to find a large number of candidate rules which can be computationally expensive.
* Calculating support is also expensive because it has to go through the entire database

## R Implementation 
Micheal Hashler, et al. has authored and maintains two very useful R packages related to association rule mining: the arules package and the arulesViz package. 

The following implementation uses the above two packages. 

https://github.com/SudhaHariharan/AprioriR

# References and useful links
1. https://www.hackerearth.com/blog/machine-learning/beginners-tutorial-apriori-algorithm-data-mining-r-implementation/
2. https://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html
